{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Universidad del Valle de Guatemala\n",
    "# (CC3094) Security Data Science\n",
    "# Laboratorio 5 - Threat Hunting\n",
    "# Santiago Taracena Puga (20017)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introducción"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El mundo digital está en constante evolución, y con ello, surgen desafíos cada vez más complejos en el ámbito de la seguridad informática. En este contexto, el Threat Hunting emerge como una disciplina esencial, destinada a identificar y neutralizar amenazas cibernéticas de manera proactiva. Este enfoque, definido por Austin Taylor como un proceso iterativo y proactivo de búsqueda de amenazas avanzadas que evaden las soluciones de seguridad tradicionales, se convierte en un pilar fundamental para salvaguardar la integridad de las redes y sistemas informáticos.\n",
    "\n",
    "El presente informe se centra en la aplicación de conocimientos interdisciplinarios, que combinan elementos de Threat Hunting, Ciencia de Datos y dominio experto, para abordar un desafío concreto: la detección de dominios maliciosos en el tráfico de red. Este ejercicio, parte del curso de Security Data Science, ofrece una oportunidad para profundizar en la comprensión de las técnicas y herramientas empleadas en la identificación temprana de potenciales amenazas digitales.\n",
    "\n",
    "El proceso de Threat Hunting, como se presenta en las directrices, es meticuloso y requiere una combinación de enfoques técnicos y experiencia humana. Desde la vasta cantidad de datos de tráfico de red, el objetivo es destilar únicamente aquellos segmentos que presenten indicios de actividad maliciosa. Este enfoque estratégico se apoya en tecnologías de Machine Learning para filtrar el tráfico benigno, y en el conocimiento experto para discernir patrones y comportamientos sospechosos que podrían pasar desapercibidos para algoritmos automatizados.\n",
    "\n",
    "El laboratorio propuesto se estructura en tres partes fundamentales: filtrado y preprocesamiento de los datos, aplicación de técnicas de Ciencia de Datos para identificar potenciales dominios generados de forma algorítmica (DGA), y finalmente, la validación de dichos dominios sospechosos a través de un análisis exhaustivo basado en dominio experto. Cada etapa de este proceso representa un eslabón crucial en la cadena de detección de amenazas, donde la intersección entre la tecnología y el juicio humano se revela como un componente indispensable.\n",
    "\n",
    "En última instancia, este informe no solo busca cumplir con los objetivos establecidos en la tarea asignada, sino también sentar las bases para una comprensión más profunda de las complejidades inherentes a la seguridad cibernética en un entorno cada vez más interconectado y dinámico."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Desarrollo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La ejecución de este laboratorio implica una serie de actividades meticulosamente diseñadas para cumplir con los objetivos establecidos. El primer paso consiste en el filtrado y preprocesamiento de los datos, donde se busca reducir el conjunto inicial de registros a una cantidad manejable para un análisis más detallado. Para ello, se inicia cargando las librerías y archivos necesarios en el entorno de trabajo, incluyendo el archivo \"large_even.json\" proporcionado, que contiene los registros de tráfico de red capturados por el IDS Suricata.\n",
    "\n",
    "Una vez cargada la información, se procede a mostrar la cantidad total de registros, confirmándose que son 746,909 en total. Dado que el interés se centra en los registros DNS, se realiza un filtro para seleccionar únicamente estos registros, lo que reduce la cantidad a 21,484, una cifra más manejable para el análisis subsiguiente.\n",
    "\n",
    "Para comprender mejor la naturaleza de los datos, se muestra la información de dos registros aleatorios, lo que proporciona una visión preliminar de la estructura del conjunto de datos. Dado que los datos están en formato JSON anidado, se utiliza la función json_normalize para normalizar la información y asignarla a un dataframe, lo que facilita su manipulación y análisis posterior.\n",
    "\n",
    "Con el dataframe normalizado en su lugar, se procede a filtrar los registros DNS para aquellos de tipo A, que mantienen una dirección IP asociada a un dominio. Este paso reduce aún más el conjunto de datos a 2,849 registros. A continuación, se realiza un filtro para obtener los dominios únicos presentes en el conjunto de datos, resultando en 177 registros únicos.\n",
    "\n",
    "Como siguiente paso, se desarrolla una función para obtener el Top Level Domain (TLD) de un dominio, esencial para el análisis posterior. Esta función utiliza la herramienta ChatGPT para su creación, asegurando la precisión en la extracción del TLD. Posteriormente, se aplica esta función al dataframe de dominios únicos para crear una nueva columna llamada \"domain_tld\", eliminando las columnas innecesarias en el proceso.\n",
    "\n",
    "En la segunda parte del laboratorio, se emplea el clasificador proporcionado para identificar los dominios generados de forma algorítmica (DGA). Este clasificador se aplica al dataframe con la columna \"domain_tld\", y se filtran aquellos dominios clasificados como DGA, teniendo en cuenta la posibilidad de falsos positivos y falsos negativos.\n",
    "\n",
    "En la tercera parte, se recurre al conocimiento experto para validar los dominios sospechosos. Se desarrolla una función que utiliza una lista de un millón de TLD proporcionada, para determinar si un TLD dado se encuentra en dicha lista. Luego, se aplica esta función para filtrar los dominios sospechosos y se procede a verificar la fecha de creación de cada uno de ellos para confirmar su naturaleza maliciosa.\n",
    "\n",
    "Este proceso sistemático y multidisciplinario, que combina técnicas de filtrado de datos, ciencia de datos y juicio experto, constituye una aproximación integral a la detección de amenazas en el tráfico de red, destacando la importancia de la colaboración entre la tecnología y el conocimiento humano en la seguridad cibernética."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parte 1 - Filtrado y Preprocesamiento"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para este ejercicio se utilizará el archivo large_eve.json que se encuentra en Canvas, en el módulo de la semana. Este archivo contiene miles de registros capturados a través del IDS Suricata. Además, se proporciona el archivo clasificador.py que recibe como parámetro un dataframe con top level domain y devuelve una clasificación DGA para cada uno de ellos (0 significa que no es DGA, y 1 que si es DGA). Este clasificador necesita el archivo d_common_en_words, también proporcionado en Canvas. Este clasificador fue analizado en la segunda semana del curso."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Cargue las librerías y archivos a utilizar en la misma ubicación."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Todos estos archivos a cargar se encuentran en las carpetas `data`, `misc`, y `models`, y son varios. Dos de los archivos a cargar son modelos en formato Pickle, por lo que una de las librerías que debemos utilizar es `pickle`, para cargar estos modelos. También necesitamos cargar un .csv con el millón de dominios más buscados en el mundo, por lo que también necesitamos importar `pandas` para cargar este archivo como dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Librerías a utilizar.\n",
    "import pandas as pd\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Con estas librerías cargadas, lo más fácil que podemos hacer para iniciar es cargar el top de dominios más buscados, ya que la carga de un archivo .csv es sumamente sencilla y un procedimiento que hemos realizado múltiples veces en el curso y más. Esto podemos realizarlo con la función `read_csv` para cargar el archivo como dataframe de pandas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rank</th>\n",
       "      <th>domain</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>google.com</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>www.google.com</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>microsoft.com</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>data.microsoft.com</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>events.data.microsoft.com</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   rank                     domain\n",
       "0     1                 google.com\n",
       "1     2             www.google.com\n",
       "2     3              microsoft.com\n",
       "3     4         data.microsoft.com\n",
       "4     5  events.data.microsoft.com"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Carga del archivo y visualización de las primeras filas.\n",
    "top_1m = pd.read_csv(\"./misc/top-1m.csv\", header=None, names=[\"rank\", \"domain\"])\n",
    "top_1m.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finalmente podemos cargar ambos modelos de Pickle utilizando la función `load` que nos da la librería mencionada y utilizando la cláusula `with` para abrir los archivos con Python. Es muy importante abrirlos en formato de lectura de bytes, ya que este es el formato de los modelos a usar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lectura del modelo de palabras comunes en inglés.\n",
    "with open(\"./models/d_common_en_words.pickle\", \"rb\") as model:\n",
    "    d_common_en_words = pickle.load(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\stara\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\base.py:318: UserWarning: Trying to unpickle estimator DecisionTreeClassifier from version 1.0.2 when using version 1.2.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Lectura del modelo de Pickle.\n",
    "with open(\"./models/Pickle_RL_Model.pkl\", \"rb\") as rl_model:\n",
    "    pickle_rl_model = pickle.load(rl_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Con esto tenemos todos los archivos necesarios almacenados. Realmente hace falta el dataset final a utilizar, el cual es un archivo en formato .json, por lo que el siguiente objetivo a realizar es la carga de este archivo con la librería necesaria."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Cargue la información del archivo large_even.json en una lista, muestre la cantidad de registros total (deben ser 746,909). Este es nuestro tráfico inicial."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Necesitamos cargar el dataset a utilizar, el cual se encuentra en formato .json, por lo que la librería que necesitamos utilizar es la librería `json` para cargar estos archivos en Python. Lo primero que debemos hacer es importar esta librería."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Librería para cargar archivos .json.\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Con esta librería presente y utilizable en el código, podemos proceder a leer el archivo y cargarlo como formato de lista de diccionarios en el cuaderno. Hay un pequeño problema al momento de hacer esta acción, y es que el archivo está compuesto de JSONs individuales y no está en formato de lista ni está separado por comas, por lo que debemos hacer es iterar a través de todo el archivo con el objetivo de parsear cada línea como un JSON individual y agregarlo a una lista de JSONs que sea manejable. La carga de cada línea por individual la podemos hacer utilizando la función `loads` que viene con la librería para el manejo de JSONs de Python, y posteriormente podemos agregar cada JSON parseado a una lista y utilizar esta lista como dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lista inicial a utilizar.\n",
    "large_eve = list()\n",
    "\n",
    "# Lectura, iteración y parseo de los JSON individuales.\n",
    "with open(\"./data/large_eve.json\", \"r\") as file:\n",
    "    for line in file:\n",
    "        json_file = json.loads(line)\n",
    "        large_eve.append(json_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finalmente, con la lista finalizada, podemos proceder a verificar la cantidad de registros en el dataset que se mencionó anteriormente utilizando la función `len` para obtener la cantidad de JSONs que fueron almacenados en la lista que utilizamos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "746909"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Registros en la lista large_eve.\n",
    "len(large_eve)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Con la carga de datos realizada, podemos observar que efectivamente tenemos 746,909 registros a utilizar para el análisis. Sin embargo, vale la pena analizar que estos son registros de múltiples tipos, y no sólo de tipo DNS, por lo que el siguiente paso a realizar es filtrar todos los registros DNS."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Debido a que estamos buscando dominios web, del total de registros, solamente estamos interesados en los registros DNS. Cargue únicamente aquellos registros que sean DNS."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La columna del dataset que representa el tipo de los registros es \"event_type\". Por esta razón, debemos iterar sobre los registros que tenemos actualmente y obtener únicamente los que tengan como propiedad de esa columna el string \"dns\" que indica que el registro es del tipo que necesitamos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'timestamp': '2017-07-22T19:47:38.146953-0500',\n",
       " 'flow_id': 2199315651640391,\n",
       " 'pcap_cnt': 4452918,\n",
       " 'event_type': 'dns',\n",
       " 'vlan': 130,\n",
       " 'src_ip': '192.168.207.4',\n",
       " 'src_port': 53,\n",
       " 'dest_ip': '192.168.203.67',\n",
       " 'dest_port': 50975,\n",
       " 'proto': 'UDP',\n",
       " 'dns': {'type': 'answer',\n",
       "  'id': 35892,\n",
       "  'rcode': 'NXDOMAIN',\n",
       "  'rrname': '<root>',\n",
       "  'rrtype': 'SOA',\n",
       "  'ttl': 20864}}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Lista con los registros DNS.\n",
    "large_eve_dns = list()\n",
    "\n",
    "# Iteración y obtención de los registros DNS.\n",
    "for item in large_eve:\n",
    "    if (item[\"event_type\"] == \"dns\"):\n",
    "        large_eve_dns.append(item)\n",
    "\n",
    "# Retorno del último elemento para verificar la columna DNS.\n",
    "large_eve_dns[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos observar que efectivamente hemos obtenido los registros cuyo tipo es DNS. Al observar el JSON que se encuentra al final de nuestro listado de registros de este tipo, podemos ver cómo la propiedad de \"event_type\" es DNS, justo como lo necesitábamos. El resto de registros también son de este tipo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Muestre la nueva cantidad de registros filtrados. Deben ser 21484. Esta es una cantidad mucho más manejable, pero aún se debe seguir depurando la información a buscar."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ya que tenemos nuestra lista de Python con todos los registros de tipo DNS disponible, podemos únicamente utilizar la función `len` para obtener cuántos registros de este tipo hay. Básicamente utilizamos la misma lógica que se utilizó para observar los registros totales."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15749"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Cantidad de registros de tipo DNS.\n",
    "len(large_eve_dns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Los registros no son exactamente los 21,484 que se habían mencionado inicialmente en las instrucciones, pero no hay mucho problema, ya que igual tenemos una gran cantidad de registros útiles a utilizar para el resto del laboratorio. La muestra es lo suficientemente significativa para los procedimientos que continúan."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Muestre la información de 2 registros cualesquiera."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ya que tenemos el dataset filtrado con los 15,749 registros que resultaron del procedimiento de obtener sólo los registros que eran de tipo DNS, podemos retornar por ejemplo, los dos primeros elementos de la lista aprovechando la sintáxis de Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'timestamp': '2017-07-22T17:33:16.661646-0500',\n",
       "  'flow_id': 1327836194150542,\n",
       "  'pcap_cnt': 22269,\n",
       "  'event_type': 'dns',\n",
       "  'vlan': 110,\n",
       "  'src_ip': '2001:0dbb:0c18:0011:0260:6eff:fe30:0863',\n",
       "  'src_port': 59680,\n",
       "  'dest_ip': '2001:0500:0001:0000:0000:0000:803f:0235',\n",
       "  'dest_port': 53,\n",
       "  'proto': 'UDP',\n",
       "  'dns': {'type': 'query',\n",
       "   'id': 15529,\n",
       "   'rrname': 'api.wunderground.com',\n",
       "   'rrtype': 'A',\n",
       "   'tx_id': 0}},\n",
       " {'timestamp': '2017-07-22T17:33:24.990320-0500',\n",
       "  'flow_id': 2022925111925872,\n",
       "  'pcap_cnt': 54352,\n",
       "  'event_type': 'dns',\n",
       "  'vlan': 110,\n",
       "  'src_ip': '2001:0dbb:0c18:0011:0260:6eff:fe30:0863',\n",
       "  'src_port': 38051,\n",
       "  'dest_ip': '2001:0500:0003:0000:0000:0000:0000:0042',\n",
       "  'dest_port': 53,\n",
       "  'proto': 'UDP',\n",
       "  'dns': {'type': 'query',\n",
       "   'id': 58278,\n",
       "   'rrname': 'stork79.dropbox.com',\n",
       "   'rrtype': 'A',\n",
       "   'tx_id': 0}}]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Primeros dos elementos de la lista.\n",
    "large_eve_dns[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos observar la estructura de estos registros. Vemos que efectivamente el tipo que se encuentra en la columna \"event_type\" es DNS, y también podemos ver la columna \"dns\" que nos indica múltiples características importantes del registro."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. Debido a que la data consiste en json anidados, utilice la característica json_normalize para normalizar la información y asignarla en un dataframe. Muestre el shape del dataframe, debería obtener (21484, 163)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos a utilizar la función `json_normalize` para convertir esta lista llena de registros en formato JSON a un dataframe de pandas. Para utilizar esta función, lo primero que debemos hacer claramente es importar la función."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instrucción para importar json_normalize.\n",
    "from pandas import json_normalize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Con la función importada, únicamente resta usarla pasando como parámetro la lista de JSONs que tenemos como dataset. La función se va a encargar de pasar la lista a un dataframe mucho más fácil de manejar y utilizar en entornos como este. Al tener nuestro dataframe listo, podemos proceder a observar cuántas filas y cuántas columnas tiene el mismo utilizando la propiedad `shape` que tienen los dataframes de pandas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(15749, 18)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Conversión a dataframe y tupla dimensional.\n",
    "dataframe = json_normalize(large_eve_dns)\n",
    "dataframe.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos observar cómo el dataset tiene las mismas 15,749 filas con las que contábamos anteriormente, y ahora posee 18 columnas provenientes de las propiedades que tenía cada archivo JSON que se encontraba en la lista. Veamos mejor el dataset con `head` para ver las primeras cinco filas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>timestamp</th>\n",
       "      <th>flow_id</th>\n",
       "      <th>pcap_cnt</th>\n",
       "      <th>event_type</th>\n",
       "      <th>vlan</th>\n",
       "      <th>src_ip</th>\n",
       "      <th>src_port</th>\n",
       "      <th>dest_ip</th>\n",
       "      <th>dest_port</th>\n",
       "      <th>proto</th>\n",
       "      <th>dns.type</th>\n",
       "      <th>dns.id</th>\n",
       "      <th>dns.rrname</th>\n",
       "      <th>dns.rrtype</th>\n",
       "      <th>dns.tx_id</th>\n",
       "      <th>dns.rcode</th>\n",
       "      <th>dns.ttl</th>\n",
       "      <th>dns.rdata</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2017-07-22T17:33:16.661646-0500</td>\n",
       "      <td>1327836194150542</td>\n",
       "      <td>22269</td>\n",
       "      <td>dns</td>\n",
       "      <td>110</td>\n",
       "      <td>2001:0dbb:0c18:0011:0260:6eff:fe30:0863</td>\n",
       "      <td>59680</td>\n",
       "      <td>2001:0500:0001:0000:0000:0000:803f:0235</td>\n",
       "      <td>53</td>\n",
       "      <td>UDP</td>\n",
       "      <td>query</td>\n",
       "      <td>15529</td>\n",
       "      <td>api.wunderground.com</td>\n",
       "      <td>A</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2017-07-22T17:33:24.990320-0500</td>\n",
       "      <td>2022925111925872</td>\n",
       "      <td>54352</td>\n",
       "      <td>dns</td>\n",
       "      <td>110</td>\n",
       "      <td>2001:0dbb:0c18:0011:0260:6eff:fe30:0863</td>\n",
       "      <td>38051</td>\n",
       "      <td>2001:0500:0003:0000:0000:0000:0000:0042</td>\n",
       "      <td>53</td>\n",
       "      <td>UDP</td>\n",
       "      <td>query</td>\n",
       "      <td>58278</td>\n",
       "      <td>stork79.dropbox.com</td>\n",
       "      <td>A</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2017-07-22T17:33:27.379891-0500</td>\n",
       "      <td>578544790391795</td>\n",
       "      <td>54519</td>\n",
       "      <td>dns</td>\n",
       "      <td>150</td>\n",
       "      <td>192.168.205.170</td>\n",
       "      <td>31393</td>\n",
       "      <td>192.168.207.4</td>\n",
       "      <td>53</td>\n",
       "      <td>UDP</td>\n",
       "      <td>query</td>\n",
       "      <td>54724</td>\n",
       "      <td>hpca-tier2.office.aol.com.ad.aol.aoltw.net</td>\n",
       "      <td>A</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2017-07-22T17:33:27.380146-0500</td>\n",
       "      <td>578544790391795</td>\n",
       "      <td>54520</td>\n",
       "      <td>dns</td>\n",
       "      <td>150</td>\n",
       "      <td>192.168.207.4</td>\n",
       "      <td>53</td>\n",
       "      <td>192.168.205.170</td>\n",
       "      <td>31393</td>\n",
       "      <td>UDP</td>\n",
       "      <td>answer</td>\n",
       "      <td>54724</td>\n",
       "      <td>hpca-tier2.office.aol.com.ad.aol.aoltw.net</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NXDOMAIN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2017-07-22T17:33:27.380146-0500</td>\n",
       "      <td>578544790391795</td>\n",
       "      <td>54520</td>\n",
       "      <td>dns</td>\n",
       "      <td>150</td>\n",
       "      <td>192.168.207.4</td>\n",
       "      <td>53</td>\n",
       "      <td>192.168.205.170</td>\n",
       "      <td>31393</td>\n",
       "      <td>UDP</td>\n",
       "      <td>answer</td>\n",
       "      <td>54724</td>\n",
       "      <td>&lt;root&gt;</td>\n",
       "      <td>SOA</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NXDOMAIN</td>\n",
       "      <td>20864.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         timestamp           flow_id  pcap_cnt event_type  \\\n",
       "0  2017-07-22T17:33:16.661646-0500  1327836194150542     22269        dns   \n",
       "1  2017-07-22T17:33:24.990320-0500  2022925111925872     54352        dns   \n",
       "2  2017-07-22T17:33:27.379891-0500   578544790391795     54519        dns   \n",
       "3  2017-07-22T17:33:27.380146-0500   578544790391795     54520        dns   \n",
       "4  2017-07-22T17:33:27.380146-0500   578544790391795     54520        dns   \n",
       "\n",
       "   vlan                                   src_ip  src_port  \\\n",
       "0   110  2001:0dbb:0c18:0011:0260:6eff:fe30:0863     59680   \n",
       "1   110  2001:0dbb:0c18:0011:0260:6eff:fe30:0863     38051   \n",
       "2   150                          192.168.205.170     31393   \n",
       "3   150                            192.168.207.4        53   \n",
       "4   150                            192.168.207.4        53   \n",
       "\n",
       "                                   dest_ip  dest_port proto dns.type  dns.id  \\\n",
       "0  2001:0500:0001:0000:0000:0000:803f:0235         53   UDP    query   15529   \n",
       "1  2001:0500:0003:0000:0000:0000:0000:0042         53   UDP    query   58278   \n",
       "2                            192.168.207.4         53   UDP    query   54724   \n",
       "3                          192.168.205.170      31393   UDP   answer   54724   \n",
       "4                          192.168.205.170      31393   UDP   answer   54724   \n",
       "\n",
       "                                   dns.rrname dns.rrtype  dns.tx_id dns.rcode  \\\n",
       "0                        api.wunderground.com          A        0.0       NaN   \n",
       "1                         stork79.dropbox.com          A        0.0       NaN   \n",
       "2  hpca-tier2.office.aol.com.ad.aol.aoltw.net          A        0.0       NaN   \n",
       "3  hpca-tier2.office.aol.com.ad.aol.aoltw.net        NaN        NaN  NXDOMAIN   \n",
       "4                                      <root>        SOA        NaN  NXDOMAIN   \n",
       "\n",
       "   dns.ttl dns.rdata  \n",
       "0      NaN       NaN  \n",
       "1      NaN       NaN  \n",
       "2      NaN       NaN  \n",
       "3      NaN       NaN  \n",
       "4  20864.0       NaN  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Primeras cinco filas del dataframe.\n",
    "dataframe.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tenemos las mismas columnas que constituían las propiedades de cada JSON, y podemos observar cómo la propiedad \"dns\" ahora tiene múltiples columnas con el prefijo \"dns\" para representar las propiedades del objeto que se encontraba adentro de la columna."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7. Como estamos buscando dominios DGA, debemos filtrar los registros DNS para aquellos registros tipo A (son aquellos que mantienen una dirección IP asociada a un dominio). Después de filtrar debería obtener 2849 registros."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Filtrar registros utilizando pandas y sus dataframes es mucho más fácil que hacerlo utilizando el listado de JSONs que teníamos anteriormente. Lo único que debemos hacer es seleccionar los datos donde la columna \"dns.rrtype\" sea igual al tipo A que estamos buscando. Con esta consulta finalizada podemos observar la forma del dataframe resultante y ver los 2,849 registros que son de tipo A en el dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2849, 18)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Filtrado de los registros DNS de tipo A.\n",
    "dga_dataframe = dataframe.loc[dataframe[\"dns.rrtype\"] == \"A\", :]\n",
    "dga_dataframe.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Efectivamente, tenemos 2,849 registros de tipo A. Con estos registros podemos comenzar a analizar los dominios que se encuentran en cada una de las filas que obtuvimos luego del proceso de filtrado."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8. Filtre los dominios únicos. Debe obtener 177 registros únicos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para filtrar la cantidad de registros únicos podemos utilizar la función `unique` que efectivamente nos retorna un listado de valores únicos que se encuentran en la columna sobre la que estamos haciendo la llamada de la función. La longitud de esta lista nos dice cuántos registros únicos hay."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "177"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Cantidad de dominios únicos en el dataset.\n",
    "len(dga_dataframe[\"dns.rrname\"].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos observar que, efectivamente, tenemos 177 registros únicos en lo que respecta a dominios a investigar por medio de los modelos que se utilizarán posteriormente. Procederemos a analizar estos 177 dominios con múltiples técnicas vistas en clase."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "9. Escriba una función que obtenga el TLD para un dominio. Por ejemplo, para api.wunderground.com el TLD es wunderground.com, para safebrowsing.clients.google.com.home, el TLD es home. Utilice ChatGPT para esta función, verifique que obtiene correctamente el TLD, incluya el prompt utilizado en su notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos desarrollar una función que obtenga el TLD de un dominio utilizando expresiones regulares. Para poder utilizar este tipo de expresiones en Python debemos hacer uso del paquete `re`, que se dedica a utilizar y trabajar con expresiones regulares. La expresión regular que nos indica el TLD del dominio es `r\"\\.([^.]+)$\"`, ya que obtiene la última cadena de strings luego del último punto del dominio, lo que efectivamente nos permite obtener el TLD que necesitamos para seguir trabajando.\n",
    "\n",
    "El prompt que se utilizó para generar la función con ChatGPT fue: \"I need a Python function that gets a domain's TLD. For instance, if I pass \"api.wunderground.com\" as a parameter, the function should return \".com\" and if I pass \"safebrowsing.clients.google.com.home\" as a parameter, it should return \".home\".\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TLD de \"api.wunderground.com\": com\n",
      "TLD de \"safebrowsing.clients.google.com.home\": home\n"
     ]
    }
   ],
   "source": [
    "# Paquete para usar expresiones regulares.\n",
    "import re\n",
    "\n",
    "# Función get_tld, que obtiene el TLD de un dominio.\n",
    "def get_tld(domain):\n",
    "\n",
    "    # Expresión regular para encontrar el TLD.\n",
    "    tld_regex = r\"\\.([^.]+)$\"\n",
    "\n",
    "    # Búsqueda del TLD en el dominio utilizando la expresión regular.\n",
    "    match = re.search(tld_regex, domain)\n",
    "\n",
    "    # Verificar si se encontró el TLD y devolverlo.\n",
    "    return match.group(1) if (match) else None\n",
    "\n",
    "# Casos de uso mencionados.\n",
    "print(f\"TLD de \\\"api.wunderground.com\\\": {get_tld('api.wunderground.com')}\")\n",
    "print(f\"TLD de \\\"safebrowsing.clients.google.com.home\\\": {get_tld('safebrowsing.clients.google.com.home')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Al hacer uso de la función que nos generó ChatGPT con los dos casos de uso que se colocaron como ejemplo, podemos observar que efectivamente la función se encuentra funcionando correctamente. Con esta función ya podemos comenzar a obtener el TLD de los dominios y utilizarlo para clasificar los mismos dominios."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "10. Del dataframe de dominios únicos de tipo A, obtenga el TLD (top level domain) utilizando la función anterior para crear una columna nueva llamada domain_tld, y elimine todas las demás columnas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El procedimiento de obtener los TLD de cada dominio es sencillo si ya contamos con una función para obtener los mismos TLDs necesarios. Lo único que debemos realizar es aplicar la función `apply` sobre la columna que tiene cada dominio, y asignar el resultado de esta aplicación pasando la función `get_tld` a una nueva columna del dataset. Como se mencionaba, esta columna va a recibir el nombre de \"domain_tld\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\stara\\AppData\\Local\\Temp\\ipykernel_13520\\3674691215.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  dga_dataframe[\"domain_tld\"] = dga_dataframe[\"dns.rrname\"].apply(get_tld)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>domain_tld</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>com</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>com</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>net</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>com</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>home</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  domain_tld\n",
       "0        com\n",
       "1        com\n",
       "2        net\n",
       "5        com\n",
       "6       home"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Obtener el TLD para cada dominio único de tipo A y crear una nueva columna domain_tld.\n",
    "dga_dataframe[\"domain_tld\"] = dga_dataframe[\"dns.rrname\"].apply(get_tld)\n",
    "\n",
    "# Eliminar todas las demás columnas.\n",
    "domain_tld_dataframe = dga_dataframe[[\"domain_tld\"]]\n",
    "\n",
    "# Mostrar los primeros registros del DataFrame resultante.\n",
    "domain_tld_dataframe.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ya tenemos el dataset con los TLDs de cada uno de los dominios. Con este dataset podemos comenzar a aplicar técnicas de ciencia de datos para poder clasificar cada uno de los TLDs, y de esta forma obtener si los mismos son DGA o no lo son."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parte 2 - Data Science"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En la parte 2 del presente laboratorio se empiezan a utilizar técnicas de ciencia de datos con el objetivo de verificar si los dominios son DGA o no lo son. El primer paso a realizar es utilizar un modelo de clasificación que aplica técnicas de análisis de entropía de la información, entre más, para poder obtener si el dominio es DGA o no dado el TLD que se obtuvo en la sección anterior del laboratorio. Este modelo se puede utilizar con el archivo de Python que fue proporcionado junto con el resto del material para realizar el laboratorio."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "11. Utilice el clasificador proporcionado, debe pasarle como parámetro el dataframe con la columna domain_tld, y asignar el resultado a un nuevo dataframe."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El clasificador que se menciona se encuentra en el archivo `clasificador.py`, que por motivos de organización fue colocado en la carpeta \"utils\" del proyecto. Al importar este archivo necesitamos específicamente la función `clasificacion` para poder realizar este procedimiento, por lo que lo primero que debemos hacer es importar esta función."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Función clasificacion para clasificar los dominios por su TLD.\n",
    "from utils.clasificador import clasificacion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Con esta función importada, debemos realizar un proceso antes de utilizarla. Debemos limpiar algunos valores que no resultan útiles y se encuentran en el dataset con los dominios. Algunos TLDs resultaron ser None y otros resultaron en strings vacíos, por lo que se va a limpiar el dataset de estos valores verificando que el valor no sea None utilizando la función `notnull`, y también verificando con una simple condición que el valor de la columna no sea un string vacío."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\stara\\AppData\\Local\\Temp\\ipykernel_13520\\2202521977.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  domain_tld_dataframe[\"domain_tld\"].fillna(\"\", inplace=True)\n"
     ]
    }
   ],
   "source": [
    "# Limpieza de los TLDs que no son útiles.\n",
    "domain_tld_dataframe[\"domain_tld\"].fillna(\"\", inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Luego de haber filtrado los registros que no eran útiles e incluso podían llegar a dar errores al momento de ejecutar el código, podemos finalmente utilizar la función `clasificacion` con el objetivo de obtener un nuevo dataset que nos diga si los dominios son DGA o no lo son."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\stara\\OneDrive\\Escritorio\\Semestre IX\\Security Data Science\\Laboratorios\\Laboratorio 5\\sds-lab-05\\src\\utils\\clasificador.py:113: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[\"length\"] = df[\"domain_tld\"].str.len()\n",
      "c:\\Users\\stara\\OneDrive\\Escritorio\\Semestre IX\\Security Data Science\\Laboratorios\\Laboratorio 5\\sds-lab-05\\src\\utils\\clasificador.py:114: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[\"digits\"] = df[\"domain_tld\"].str.count(\"[0-9]\")\n",
      "c:\\Users\\stara\\OneDrive\\Escritorio\\Semestre IX\\Security Data Science\\Laboratorios\\Laboratorio 5\\sds-lab-05\\src\\utils\\clasificador.py:115: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[\"entropy\"] = df[\"domain_tld\"].apply(H_entropy)\n",
      "c:\\Users\\stara\\OneDrive\\Escritorio\\Semestre IX\\Security Data Science\\Laboratorios\\Laboratorio 5\\sds-lab-05\\src\\utils\\clasificador.py:116: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[\"vowel-cons\"] = df[\"domain_tld\"].apply(vowel_consonant_ratio)\n",
      "c:\\Users\\stara\\OneDrive\\Escritorio\\Semestre IX\\Security Data Science\\Laboratorios\\Laboratorio 5\\sds-lab-05\\src\\utils\\clasificador.py:117: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[\"firstDigitIndex\"] = df[\"domain_tld\"].apply(firstDigitIndex)\n",
      "c:\\Users\\stara\\OneDrive\\Escritorio\\Semestre IX\\Security Data Science\\Laboratorios\\Laboratorio 5\\sds-lab-05\\src\\utils\\clasificador.py:122: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[\"ngrams\"] = df[\"domain_tld\"].apply(\n",
      "c:\\Users\\stara\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\base.py:318: UserWarning: Trying to unpickle estimator DecisionTreeClassifier from version 1.0.2 when using version 1.2.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n",
      "c:\\Users\\stara\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\utils\\validation.py:767: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if not hasattr(array, \"sparse\") and array.dtypes.apply(is_sparse).any():\n",
      "c:\\Users\\stara\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\utils\\validation.py:605: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if is_sparse(pd_dtype):\n",
      "c:\\Users\\stara\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\utils\\validation.py:614: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if is_sparse(pd_dtype) or not is_extension_array_dtype(pd_dtype):\n"
     ]
    }
   ],
   "source": [
    "# Nuevo dataset que nos dice si un dominio es DGA o no.\n",
    "domain_tld_dga_dataframe = clasificacion(domain_tld_dataframe)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Luego de realizar la clasificación hemos obtenido una columna \"isDGA\" que nos indica si el dominio es DGA o no lo es por medio de la aplicación de la misma clasificación mencionada. El siguiente paso es filtrar los dominios que resultaron ser DGA según la misma."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "12. Filtre aquellos considerados como DGA (valor 1 ) y muéstrelos. Recuerde que los modelos de ML ofrecen una predicción, pero los resultados pueden incluir falsos positivos y falsos negativos, por lo que no podemos fiarnos por completo de esta clasificación y debemos seguir indagando. Después de eliminar duplicados, debe obtener 61 registros únicos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ya con el nuevo dataset obtenido correctamente, podemos observar la columna \"isDGA\" que nos dice si el dominio es DGA con un 1, o si no lo es con un 0. Para obtener los TLD de los dominios que son DGA únicamente debemos filtrar los que tienen un 1 en la columna mencionada."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>domain_tld</th>\n",
       "      <th>isDGA</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>163</th>\n",
       "      <td>110phpmyadmin</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>290</th>\n",
       "      <td>110phpmyadmin</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        domain_tld  isDGA\n",
       "163  110phpmyadmin      1\n",
       "290  110phpmyadmin      1"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Dominios que se clasificaron como DGA.\n",
    "domain_tld_dga_dataframe[domain_tld_dga_dataframe[\"isDGA\"] == 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos observar que los dominios que el clasificador coloca como DGA son dominios cuyo TLD es \"110phpmyadmin\" el cual efectivamente es un dominio bastante extraño. Procedamos a adjuntar esta columna en el dataframe inicial para tenerla como referencia."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\stara\\AppData\\Local\\Temp\\ipykernel_13520\\4272931137.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  dga_dataframe[\"is_dga\"] = domain_tld_dga_dataframe[\"isDGA\"]\n"
     ]
    }
   ],
   "source": [
    "# Copia de la columna isDGA al dataframe inicial.\n",
    "dga_dataframe[\"is_dga\"] = domain_tld_dga_dataframe[\"isDGA\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Con la columna adjuntada como \"is_dga\" en el dataframe que tiene básicamente toda la información que hemos estado necesitando, podemos observar si este procedimiento se realizó correctamente mostrando las primeras cinco filas del mismo dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>timestamp</th>\n",
       "      <th>flow_id</th>\n",
       "      <th>pcap_cnt</th>\n",
       "      <th>event_type</th>\n",
       "      <th>vlan</th>\n",
       "      <th>src_ip</th>\n",
       "      <th>src_port</th>\n",
       "      <th>dest_ip</th>\n",
       "      <th>dest_port</th>\n",
       "      <th>proto</th>\n",
       "      <th>dns.type</th>\n",
       "      <th>dns.id</th>\n",
       "      <th>dns.rrname</th>\n",
       "      <th>dns.rrtype</th>\n",
       "      <th>dns.tx_id</th>\n",
       "      <th>dns.rcode</th>\n",
       "      <th>dns.ttl</th>\n",
       "      <th>dns.rdata</th>\n",
       "      <th>domain_tld</th>\n",
       "      <th>is_dga</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2017-07-22T17:33:16.661646-0500</td>\n",
       "      <td>1327836194150542</td>\n",
       "      <td>22269</td>\n",
       "      <td>dns</td>\n",
       "      <td>110</td>\n",
       "      <td>2001:0dbb:0c18:0011:0260:6eff:fe30:0863</td>\n",
       "      <td>59680</td>\n",
       "      <td>2001:0500:0001:0000:0000:0000:803f:0235</td>\n",
       "      <td>53</td>\n",
       "      <td>UDP</td>\n",
       "      <td>query</td>\n",
       "      <td>15529</td>\n",
       "      <td>api.wunderground.com</td>\n",
       "      <td>A</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>com</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2017-07-22T17:33:24.990320-0500</td>\n",
       "      <td>2022925111925872</td>\n",
       "      <td>54352</td>\n",
       "      <td>dns</td>\n",
       "      <td>110</td>\n",
       "      <td>2001:0dbb:0c18:0011:0260:6eff:fe30:0863</td>\n",
       "      <td>38051</td>\n",
       "      <td>2001:0500:0003:0000:0000:0000:0000:0042</td>\n",
       "      <td>53</td>\n",
       "      <td>UDP</td>\n",
       "      <td>query</td>\n",
       "      <td>58278</td>\n",
       "      <td>stork79.dropbox.com</td>\n",
       "      <td>A</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>com</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2017-07-22T17:33:27.379891-0500</td>\n",
       "      <td>578544790391795</td>\n",
       "      <td>54519</td>\n",
       "      <td>dns</td>\n",
       "      <td>150</td>\n",
       "      <td>192.168.205.170</td>\n",
       "      <td>31393</td>\n",
       "      <td>192.168.207.4</td>\n",
       "      <td>53</td>\n",
       "      <td>UDP</td>\n",
       "      <td>query</td>\n",
       "      <td>54724</td>\n",
       "      <td>hpca-tier2.office.aol.com.ad.aol.aoltw.net</td>\n",
       "      <td>A</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>net</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2017-07-22T17:33:36.672785-0500</td>\n",
       "      <td>237919524635665</td>\n",
       "      <td>55496</td>\n",
       "      <td>dns</td>\n",
       "      <td>110</td>\n",
       "      <td>2001:0dbb:0c18:0011:0260:6eff:fe30:0863</td>\n",
       "      <td>41663</td>\n",
       "      <td>2001:07fd:0000:0000:0000:0000:0000:0001</td>\n",
       "      <td>53</td>\n",
       "      <td>UDP</td>\n",
       "      <td>query</td>\n",
       "      <td>45082</td>\n",
       "      <td>api.wunderground.com</td>\n",
       "      <td>A</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>com</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2017-07-22T17:33:38.537426-0500</td>\n",
       "      <td>2167545251640146</td>\n",
       "      <td>55687</td>\n",
       "      <td>dns</td>\n",
       "      <td>180</td>\n",
       "      <td>192.168.198.62</td>\n",
       "      <td>35092</td>\n",
       "      <td>192.168.207.4</td>\n",
       "      <td>53</td>\n",
       "      <td>UDP</td>\n",
       "      <td>query</td>\n",
       "      <td>7425</td>\n",
       "      <td>safebrowsing.clients.google.com.home</td>\n",
       "      <td>A</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>home</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         timestamp           flow_id  pcap_cnt event_type  \\\n",
       "0  2017-07-22T17:33:16.661646-0500  1327836194150542     22269        dns   \n",
       "1  2017-07-22T17:33:24.990320-0500  2022925111925872     54352        dns   \n",
       "2  2017-07-22T17:33:27.379891-0500   578544790391795     54519        dns   \n",
       "5  2017-07-22T17:33:36.672785-0500   237919524635665     55496        dns   \n",
       "6  2017-07-22T17:33:38.537426-0500  2167545251640146     55687        dns   \n",
       "\n",
       "   vlan                                   src_ip  src_port  \\\n",
       "0   110  2001:0dbb:0c18:0011:0260:6eff:fe30:0863     59680   \n",
       "1   110  2001:0dbb:0c18:0011:0260:6eff:fe30:0863     38051   \n",
       "2   150                          192.168.205.170     31393   \n",
       "5   110  2001:0dbb:0c18:0011:0260:6eff:fe30:0863     41663   \n",
       "6   180                           192.168.198.62     35092   \n",
       "\n",
       "                                   dest_ip  dest_port proto dns.type  dns.id  \\\n",
       "0  2001:0500:0001:0000:0000:0000:803f:0235         53   UDP    query   15529   \n",
       "1  2001:0500:0003:0000:0000:0000:0000:0042         53   UDP    query   58278   \n",
       "2                            192.168.207.4         53   UDP    query   54724   \n",
       "5  2001:07fd:0000:0000:0000:0000:0000:0001         53   UDP    query   45082   \n",
       "6                            192.168.207.4         53   UDP    query    7425   \n",
       "\n",
       "                                   dns.rrname dns.rrtype  dns.tx_id dns.rcode  \\\n",
       "0                        api.wunderground.com          A        0.0       NaN   \n",
       "1                         stork79.dropbox.com          A        0.0       NaN   \n",
       "2  hpca-tier2.office.aol.com.ad.aol.aoltw.net          A        0.0       NaN   \n",
       "5                        api.wunderground.com          A        0.0       NaN   \n",
       "6        safebrowsing.clients.google.com.home          A        0.0       NaN   \n",
       "\n",
       "   dns.ttl dns.rdata domain_tld  is_dga  \n",
       "0      NaN       NaN        com       0  \n",
       "1      NaN       NaN        com       0  \n",
       "2      NaN       NaN        net       0  \n",
       "5      NaN       NaN        com       0  \n",
       "6      NaN       NaN       home       0  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Cinco primeras filas del dataframe inicial.\n",
    "dga_dataframe.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finalmente vale mucho la pena corroborar que el mapeo de la columna se haya realizado correctamente revisando cuáles son las dos entradas que marcan que sí son DGA. El filtrado es el mismo de la vez pasada, sólo que con el dataframe original."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>timestamp</th>\n",
       "      <th>flow_id</th>\n",
       "      <th>pcap_cnt</th>\n",
       "      <th>event_type</th>\n",
       "      <th>vlan</th>\n",
       "      <th>src_ip</th>\n",
       "      <th>src_port</th>\n",
       "      <th>dest_ip</th>\n",
       "      <th>dest_port</th>\n",
       "      <th>proto</th>\n",
       "      <th>dns.type</th>\n",
       "      <th>dns.id</th>\n",
       "      <th>dns.rrname</th>\n",
       "      <th>dns.rrtype</th>\n",
       "      <th>dns.tx_id</th>\n",
       "      <th>dns.rcode</th>\n",
       "      <th>dns.ttl</th>\n",
       "      <th>dns.rdata</th>\n",
       "      <th>domain_tld</th>\n",
       "      <th>is_dga</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>163</th>\n",
       "      <td>2017-07-22T17:37:31.536277-0500</td>\n",
       "      <td>363487203897045</td>\n",
       "      <td>76982</td>\n",
       "      <td>dns</td>\n",
       "      <td>110</td>\n",
       "      <td>192.168.201.68</td>\n",
       "      <td>55728</td>\n",
       "      <td>192.168.207.4</td>\n",
       "      <td>53</td>\n",
       "      <td>UDP</td>\n",
       "      <td>query</td>\n",
       "      <td>359</td>\n",
       "      <td>192.168.22.110phpmyadmin</td>\n",
       "      <td>A</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>110phpmyadmin</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>290</th>\n",
       "      <td>2017-07-22T17:37:31.538774-0500</td>\n",
       "      <td>820780961839254</td>\n",
       "      <td>76990</td>\n",
       "      <td>dns</td>\n",
       "      <td>110</td>\n",
       "      <td>192.168.201.68</td>\n",
       "      <td>51202</td>\n",
       "      <td>192.168.207.4</td>\n",
       "      <td>53</td>\n",
       "      <td>UDP</td>\n",
       "      <td>query</td>\n",
       "      <td>61247</td>\n",
       "      <td>192.168.22.110phpmyadmin</td>\n",
       "      <td>A</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>110phpmyadmin</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           timestamp          flow_id  pcap_cnt event_type  \\\n",
       "163  2017-07-22T17:37:31.536277-0500  363487203897045     76982        dns   \n",
       "290  2017-07-22T17:37:31.538774-0500  820780961839254     76990        dns   \n",
       "\n",
       "     vlan          src_ip  src_port        dest_ip  dest_port proto dns.type  \\\n",
       "163   110  192.168.201.68     55728  192.168.207.4         53   UDP    query   \n",
       "290   110  192.168.201.68     51202  192.168.207.4         53   UDP    query   \n",
       "\n",
       "     dns.id                dns.rrname dns.rrtype  dns.tx_id dns.rcode  \\\n",
       "163     359  192.168.22.110phpmyadmin          A        0.0       NaN   \n",
       "290   61247  192.168.22.110phpmyadmin          A        0.0       NaN   \n",
       "\n",
       "     dns.ttl dns.rdata     domain_tld  is_dga  \n",
       "163      NaN       NaN  110phpmyadmin       1  \n",
       "290      NaN       NaN  110phpmyadmin       1  "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Filas donde is_dga es igual a 1.\n",
    "dga_dataframe[dga_dataframe[\"is_dga\"] == 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Con el dataframe como lo tenemos actualmente podemos observar los dominios extraños que tienen como TLD \"110phpmyadmin\" y todas sus características. Sin embargo, esto no es suficiente para lograr la clasificación. Vamos a aplicar el top del millón de dominios más buscados en internet para realizar un último análisis, y finalmente llegar a conclusiones sólidas al respecto de la presente investigación."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "13. Ahora ya tenemos un listado de dominios reducido y considerado como sospechoso, por lo que debemos aplicar dominio experto para encontrar los verdaderos registros maliciosos. Escriba una función que utilice la lista de un millón de TLD proporcionada en Canvas, y devuelva 0 si el TLD se encuentra en la lista y 1 si no está. Utilice ChatGPT para crear dicha función, verifique que no se carga la lista cada vez que se busca un TLD. Incluya el prompt en su notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para realizar este proceso de clasificación finalmente utilizaremos la variable `top_1m`, que es la variable en la que se encuentra cargado el dataset con el millón de TLD más buscados en internet.\n",
    "\n",
    "El prompt para obtener la función desarrollada fue: \"I have the top one million most searched domains on the internet in a variable called `top_1m`. This top is loaded as a pandas DataFrame with columns \"rank\" and \"domain\". I need a function that verifies if a domain is part of the top one million that is located in the variable I just mentioned.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "0\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "# Convertir la columna \"domain\" a minúsculas para facilitar la comparación.\n",
    "top_1m[\"domain\"] = top_1m[\"domain\"].str.lower()\n",
    "\n",
    "# Función check_top_1m_tld, que verifica si el TLD está o no en el dataset.\n",
    "def check_top_1m_tld(tld):\n",
    "\n",
    "    # Conversión a minúsculas por facilidad.\n",
    "    tld = tld.lower()\n",
    "\n",
    "    # Verificar si el TLD está en la lista de un millón de TLD.\n",
    "    return int(not (tld in top_1m[\"domain\"].values))\n",
    "\n",
    "# Casos de uso.\n",
    "print(check_top_1m_tld(\"google.com\"))\n",
    "print(check_top_1m_tld(\"twitter.com\"))\n",
    "print(check_top_1m_tld(\"192.168.126.110phpmyadmin\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "14. Utilice la función para determinar si los TLD se encuentran en dicha lista. Filtre aquellos que si se encuentran. Después de eliminar duplicados, debería obtener 13 dominios."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lo primero que debemos realizar es una nueva columna que nos permita obtener si el dominio es parte del top que poseemos y que al mismo tiempo nos permita filtrar estos mismos dominios. Vamos a crear esta columna utilizando la función `apply` y aplicar la función `check_top_1m_tld` que ChatGPT desarrolló por nosotros con el objetivo de este mismo proceso de filtrado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\stara\\AppData\\Local\\Temp\\ipykernel_13520\\4107087381.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  dga_dataframe[\"is_top_1m\"] = dga_dataframe[\"dns.rrname\"].apply(check_top_1m_tld)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0, 1], dtype=int64)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Creación de la columna is_top_1m.\n",
    "dga_dataframe[\"is_top_1m\"] = dga_dataframe[\"dns.rrname\"].apply(check_top_1m_tld)\n",
    "dga_dataframe[\"is_top_1m\"].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Posteriormente podemos obtener los dominios que sí se encuentran en el dataset y observar un listado de los dominios únicos que se encuentran en el mismo. Esto lo podemos realizar filtrando con la sintáxis de pandas y utilizando la función `unique` en la columna \"dns.rrname\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['api.wunderground.com', 'fxfeeds.mozilla.com',\n",
       "       'safebrowsing.clients.google.com', 'en-us.fxfeeds.mozilla.com',\n",
       "       'time.windows.com', 'softwareupdate.vmware.com', 'portswigger.net',\n",
       "       'www.stopbadware.org', 'www.phpmyadmin.net', 'tools.google.com',\n",
       "       'teredo.ipv6.microsoft.com', 'clients1.google.com',\n",
       "       'ntp.ubuntu.com', 'data.alexa.com', 'www.postgresql.org',\n",
       "       'sourceforge.net', 'www.freepbx.org', 'www.gnu.org',\n",
       "       'www.google.com', 'freepbx.org', 'www.acunetix.com',\n",
       "       'go.microsoft.com', 'download.windowsupdate.com',\n",
       "       'www.update.microsoft.com', 'api.flickr.com',\n",
       "       'download.microsoft.com', 'api.facebook.com', 'google.com',\n",
       "       'mirrors.cat.pdx.edu', 'mirror.clarkson.edu',\n",
       "       'mirror.rackspace.com', 'mirror.ash.fastserv.com',\n",
       "       'mirrors.kernel.org', 'mirrors.liquidweb.com',\n",
       "       'mirrors.gigenet.com', 'mirrors.xmission.com', 'ftp.usf.edu',\n",
       "       'mirrors.rit.edu', 'clients5.google.com', 'www.apple.com',\n",
       "       'internalcheck.apple.com', 'clients2.google.com',\n",
       "       'configuration.apple.com', 'images.apple.com', 'news.google.com',\n",
       "       'gdata.youtube.com', 'dns.msftncsi.com', 'www.msftncsi.com',\n",
       "       'activex.microsoft.com', 'www.arduino.cc', 'gfe.nvidia.com',\n",
       "       'addons.mozilla.org', 'versioncheck.addons.mozilla.org',\n",
       "       'www.nagios.org', 'linkhelp.clients.google.com',\n",
       "       'redir.metaservices.microsoft.com', 'ocsp.verisign.com',\n",
       "       'client-software.real.com'], dtype=object)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Filtración de los dominios que se encuentran en el top.\n",
    "dga_dataframe[dga_dataframe[\"is_top_1m\"] == 0][\"dns.rrname\"].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "15. Finalmente, para confirmar los dominios maliciosos podemos buscar la fecha de creación del TLD. Cree una función qué en base al TLD, devuelva la fecha de creación de este. UtiliceChatGPT para escribir dicha función, incluya el prompt utilizado en su notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos observar la fecha de creación de los mismos registros con la columna \"timestamp\". Al tener la fecha de creación podemos evitar la creación de la función solicitada. Esta columna nos permite observar el timestamp y finalizar los demás incisos del laboratorio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        2017-07-22T17:33:16.661646-0500\n",
       "1        2017-07-22T17:33:24.990320-0500\n",
       "2        2017-07-22T17:33:27.379891-0500\n",
       "5        2017-07-22T17:33:36.672785-0500\n",
       "6        2017-07-22T17:33:38.537426-0500\n",
       "                      ...               \n",
       "15713    2017-07-22T19:37:54.912593-0500\n",
       "15716    2017-07-22T19:39:02.426497-0500\n",
       "15725    2017-07-22T19:42:21.167769-0500\n",
       "15737    2017-07-22T19:42:44.728139-0500\n",
       "15743    2017-07-22T19:47:03.372988-0500\n",
       "Name: timestamp, Length: 2847, dtype: object"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Columna timestamp con la fecha de creación del TLD.\n",
    "dga_dataframe[(dga_dataframe[\"is_dga\"] == 0)][\"timestamp\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "16. Muestre la fecha de creación para cada uno de los 13 dominios finales ¿Cuáles son los dominios que podemos confirmar como sospechosos?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Por los resultados obtenidos, existen bastantes más dominios de los 13 mencionados inicialmente. Para obtener sus nombres debemos obtener los dominios clasificados como DGA y que no se encuentran en el top millón oficial, y con esta selección obtenemos los únicos con `unique`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\stara\\AppData\\Local\\Temp\\ipykernel_13520\\1436883574.py:1: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
      "  dga_dataframe[dga_dataframe[\"is_dga\"] == 0][dga_dataframe[\"is_top_1m\"] == 1][\"dns.rrname\"].unique()\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array(['stork79.dropbox.com',\n",
       "       'hpca-tier2.office.aol.com.ad.aol.aoltw.net',\n",
       "       'safebrowsing.clients.google.com.home', 'www.metasploit.com',\n",
       "       'aolmtcmxm03.office.aol.com',\n",
       "       'aolmtcmxm02.office.aol.com.ad.aol.aoltw.net',\n",
       "       'aolmtcmxm02.office.aol.com', 'hpca-tier2.office.aol.com',\n",
       "       'aolmtcmxm03.office.aol.com.ad.aol.aoltw.net',\n",
       "       'aolmtcmxm04.office.aol.com', 'wpad.home',\n",
       "       'safebrowsing.clients.google.com.stayonline.net',\n",
       "       'aolmtcmxm04.office.aol.com.ad.aol.aoltw.net',\n",
       "       'AOLDTCMA04.ad.aol.aoltw.net.office.aol.com',\n",
       "       'AOLDTCMA04.office.aol.com', 'secure.informaction.com',\n",
       "       'secure.informaction.com.localdomain',\n",
       "       'safebrowsing.clients.google.com.localdomain', 'ueip.vmware.com',\n",
       "       '192.168.22.110phpmyadmin.localdomain', 'proxim.ntkrnlpa.info',\n",
       "       'www.offensive-security.com',\n",
       "       'www.offensive-security.com.stayonline.net',\n",
       "       'AOLDTCMA04.ad.aol.aoltw.net', 'gg.arrancar.org',\n",
       "       'www.sql-ledger.org', 'www.backtrack-linux.org',\n",
       "       'www.backtrack-linux.org.stayonline.net',\n",
       "       'en-us.start3.mozilla.com', 'www.theanime.cn', 'www.theanime.cn. ',\n",
       "       'wpad.aol.aoltw.net', 'wpad.aoltw.net',\n",
       "       'tools.google.com.ad.aol.aoltw.net',\n",
       "       'safebrowsing.clients.google.com.hackerlabs.vpn',\n",
       "       'secure.informaction.com.stayonline.net', 'wpad.ad.aol.aoltw.net',\n",
       "       'knoa-coll.ops.aol.com', 'knoa-coll.ops.aol.com.ad.aol.aoltw.net',\n",
       "       'toolbarqueries.clients.google.com',\n",
       "       'secure.informaction.com.home',\n",
       "       'toolbarqueries.clients.google.com.ad.aol.aoltw.net',\n",
       "       'secure.informaction.com.hsd1.pa.comcast.net',\n",
       "       'clients1.google.com.ad.aol.aoltw.net', 'en-us.www.mozilla.com',\n",
       "       'secure.informaction.com.hackerlabs.vpn', 'www.bigflickrfeed.com',\n",
       "       'wpad', 'safebrowsing.clients.google.com.lan',\n",
       "       'safebrowsing.clients.google.com.hsd1.pa.comcast.net',\n",
       "       'phppgadmin.sourceforge.net', '\"192.168.206.56\"',\n",
       "       'erp.acunetix.com', 'widgets.alexa.com',\n",
       "       'safebrowsing.clients.google.com.office.aol.com',\n",
       "       'www.malwarecity.com', 'www.sql-ledger.org.hsd1.pa.comcast.net',\n",
       "       'www.offensive-security.com.hsd1.pa.comcast.net',\n",
       "       'www.securityfocus.com', 'sync.xmarks.com', '192.168.26-27.0',\n",
       "       'www.cakephp.org', 'ky.hec.net', 'mirrors.adams.net',\n",
       "       'mirror.its.uidaho.edu', 'mirrors.ecvps.com', 'centos.cs.wisc.edu',\n",
       "       'centos.mirror.facebook.net', 'mirrors.easynews.com',\n",
       "       'mirrors.bluehost.com', 'centos.mirror.netriplex.com',\n",
       "       'mirror.stanford.edu', 'mirrors.tummy.com', 'mirror.hmc.edu',\n",
       "       'mirror.5ninesolutions.com', 'mirror.san.fastserv.com',\n",
       "       'updates.interworx.info', 'ftp.wallawalla.edu',\n",
       "       'mirror.sanctuaryhost.com', 'mirror.team-cymru.org',\n",
       "       'mirror.umoss.org', 'repo.genomics.upenn.edu',\n",
       "       'centos.mirrors.tds.net', 'mirror.nyi.net', 'mirror.atlantic.net',\n",
       "       'mirror.rocketinternet.net', 'FL', 'cloud.xmarks.com',\n",
       "       'www.metasploit.com.office.aol.com', 'saruman',\n",
       "       '192.168.21.1201.stayonline.net',\n",
       "       'clients2.google.com.ad.aol.aoltw.net', '192.168.21.1201', '',\n",
       "       'fileservices.me.com', 'r1s6i7.connectivity.me.com',\n",
       "       'aosnotify.me.com', 'kodapp.com', 'rc.threatspace.net',\n",
       "       'ul.backblaze.com', 'www.social-engineer.org',\n",
       "       'whitecell.localdomain', 'secure.informaction.com.office.aol.com',\n",
       "       'www.mac.com', 'idisk.mac.com', 'vtlfccmfxlkgifuf.com',\n",
       "       'linkhelp.clients.google.com.ad.aol.aoltw.net',\n",
       "       'update.macromates.com', '1922.168.22.254', '1922.168.22.254.home',\n",
       "       '192.168.21-28.0', 'ejfodfmfxlkgifuf.xyz', '192.168.21-28.0.home',\n",
       "       '192.168.22.201:', 'aoldtcmds01.office.aol.com',\n",
       "       'aoldtcmds01.office.aol.com.ad.aol.aoltw.net',\n",
       "       'ntp.ubuntu.com.localdomain', '192.168.22.201:.stayonline.net'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Dominios que son DGA y no están en el top 100.\n",
    "dga_dataframe[dga_dataframe[\"is_dga\"] == 0][dga_dataframe[\"is_top_1m\"] == 1][\"dns.rrname\"].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "17. Recuerde que los dominios DGA son conocidos por formarse de forma aleatoria: secuencias aleatorias de caracteres, no palabras. Indique que dominios sospechosos tienen este patrón y que pueden confirmarse como dominios DGA."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Teniendo en cuenta esta característica, algunos de los dominios que sin duda se ven bastante más extraños son \"hpca-tier2.office.aol.com.ad.aol.aoltw.net\", \"192.168.22.110phpmyadmin.localdomain\", \"192.168.22.201:.stayonline.net\", entre otros."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
